package com.abc.sbse.os.ts.csp.alsentity.ale.batch;

import com.abc.sbse.os.ts.csp.alsentity.ale.dom.EntityRecord;
import com.abc.sbse.os.ts.csp.alsentity.ale.dom.FieldData;
import com.zaxxer.hikari.HikariDataSource;

import org.springframework.batch.item.Chunk;
import org.springframework.batch.item.ItemWriter;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.stereotype.Component;

import javax.sql.DataSource;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.*;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicLong;
import java.util.logging.Level;
import java.util.logging.Logger;

@Component
public class EnhancedManualJDBCBatchItemWriter implements ItemWriter<EntityRecord>, AutoCloseable {
    private static final Logger logger = Logger.getLogger(EnhancedManualJDBCBatchItemWriter.class.getName());
    
    private volatile String sqlTemplate;
    private final HikariDataSource dataSource;
    private final ExecutorService executorService;
    private final int batchSize;
    private final int threadPoolSize;
    private final AtomicInteger totalProcessed = new AtomicInteger(0);
    private final AtomicLong totalProcessingTime = new AtomicLong(0);
    private final ThreadLocal<StringBuilder> queryBuilder;
    private final ThreadLocal<JdbcTemplate> jdbcTemplate;

    @Autowired
    public EnhancedManualJDBCBatchItemWriter(
            @Qualifier("batchDataSource") DataSource dataSource,
            String sqlTemplate, 
            @Value("${batch.size:1000}") int batchSize,
            @Value("${batch.thread.pool.size:#{T(java.lang.Runtime).getRuntime().availableProcessors()}}") 
            int threadPoolSize) {
        
        this.sqlTemplate = sqlTemplate;
        this.dataSource = (HikariDataSource) dataSource;
        this.batchSize = batchSize;
        this.threadPoolSize = threadPoolSize;
        
        // Initialize thread pool with custom configuration
        this.executorService = new ThreadPoolExecutor(
            threadPoolSize,            // Core pool size
            threadPoolSize,            // Maximum pool size
            60L,                       // Keep alive time
            TimeUnit.SECONDS,
            new LinkedBlockingQueue<>(2000),  // Queue capacity
            new ThreadFactory() {
                private final AtomicInteger threadCount = new AtomicInteger(1);
                @Override
                public Thread newThread(Runnable r) {
                    Thread thread = new Thread(r);
                    thread.setName("BatchWriter-" + threadCount.getAndIncrement());
                    thread.setDaemon(true);
                    return thread;
                }
            },
            new ThreadPoolExecutor.CallerRunsPolicy()  // Rejection policy
        );

        // Initialize thread local resources
        this.queryBuilder = ThreadLocal.withInitial(() -> new StringBuilder(1024));
        this.jdbcTemplate = ThreadLocal.withInitial(() -> {
            JdbcTemplate template = new JdbcTemplate(dataSource);
            template.setFetchSize(batchSize);
            template.setBatchSize(batchSize);
            return template;
        });
    }

    public synchronized void setSqlTemplate(String sqlTemplate) {
        if (sqlTemplate == null || sqlTemplate.trim().isEmpty()) {
            throw new IllegalArgumentException("SQL template cannot be null or empty");
        }
        this.sqlTemplate = sqlTemplate;
    }

    @Override
    public void write(Chunk<? extends EntityRecord> items) throws Exception {
        long startTime = System.currentTimeMillis();
        
        // Split items into sub-batches for parallel processing
        List<List<EntityRecord>> batches = splitIntoBatches(items);
        List<CompletableFuture<Void>> futures = new ArrayList<>();

        // Process each batch in parallel
        for (List<EntityRecord> batch : batches) {
            CompletableFuture<Void> future = CompletableFuture.runAsync(
                () -> processBatch(batch),
                executorService
            ).exceptionally(throwable -> {
                logger.severe("Batch processing failed: " + throwable.getMessage());
                throw new CompletionException(throwable);
            });
            futures.add(future);
        }

        try {
            // Wait for all batches to complete with timeout
            CompletableFuture.allOf(futures.toArray(new CompletableFuture[0]))
                .get(5, TimeUnit.MINUTES);
                
            // Update metrics
            long duration = System.currentTimeMillis() - startTime;
            totalProcessingTime.addAndGet(duration);
            int processed = totalProcessed.addAndGet(items.size());
            
            logPerformanceMetrics(items.size(), duration, processed);
            
        } catch (TimeoutException e) {
            logger.severe("Batch processing timed out after 5 minutes");
            throw new RuntimeException("Batch processing timeout", e);
        }
    }

    private void processBatch(List<EntityRecord> batch) {
        List<String> queries = new ArrayList<>(batch.size());
        StringBuilder builder = queryBuilder.get();
        JdbcTemplate template = jdbcTemplate.get();

        try {
            for (EntityRecord record : batch) {
                builder.setLength(0);
                queries.add(buildQuery(record, builder));
                
                if (queries.size() >= batchSize) {
                    executeBatchQueriesWithRetry(template, queries);
                    queries.clear();
                }
            }

            if (!queries.isEmpty()) {
                executeBatchQueriesWithRetry(template, queries);
            }
        } catch (Exception e) {
            logger.log(Level.SEVERE, "Error processing batch", e);
            throw new RuntimeException("Batch processing failed", e);
        }
    }

    private String buildQuery(EntityRecord record, StringBuilder builder) {
        builder.append(sqlTemplate);
        
        try {
            for (FieldData data : record.getFieldData()) {
                String sqlValue = data.getSQLString()
                    .replace("\\", "\\\\")
                    .replace("$", "\\$");
                
                int index;
                while ((index = builder.indexOf("?")) != -1) {
                    builder.replace(index, index + 1, sqlValue);
                }
            }
            
            // Remove trailing comma if present
            if (builder.charAt(builder.length() - 1) == ',') {
                builder.setLength(builder.length() - 1);
            }
            
            return builder.toString();
            
        } catch (Exception e) {
            logger.severe("Query building failed for record: " + e.getMessage());
            throw new RuntimeException("Failed to build query", e);
        }
    }

    private void executeBatchQueriesWithRetry(JdbcTemplate template, List<String> queries) {
        int retryCount = 0;
        int maxRetries = 3;
        long retryDelayMs = 1000; // Initial delay of 1 second
        
        while (retryCount < maxRetries) {
            try {
                template.batchUpdate(queries.toArray(new String[0]));
                return;
            } catch (Exception e) {
                retryCount++;
                if (retryCount == maxRetries) {
                    logger.severe("Batch execution failed after " + maxRetries + " retries");
                    throw new RuntimeException("Failed to execute batch", e);
                }
                logger.warning("Retry " + retryCount + " after batch execution failure");
                
                try {
                    // Exponential backoff
                    Thread.sleep(retryDelayMs * retryCount);
                } catch (InterruptedException ie) {
                    Thread.currentThread().interrupt();
                    throw new RuntimeException("Retry interrupted", ie);
                }
            }
        }
    }

    private List<List<EntityRecord>> splitIntoBatches(Chunk<? extends EntityRecord> items) {
        List<List<EntityRecord>> batches = new ArrayList<>();
        List<EntityRecord> currentBatch = new ArrayList<>();
        
        for (EntityRecord item : items) {
            currentBatch.add(item);
            if (currentBatch.size() >= batchSize) {
                batches.add(new ArrayList<>(currentBatch));
                currentBatch.clear();
            }
        }
        
        if (!currentBatch.isEmpty()) {
            batches.add(currentBatch);
        }
        
        return batches;
    }

    private void logPerformanceMetrics(int batchSize, long duration, int totalProcessed) {
        double avgTimePerRecord = totalProcessingTime.get() / (double) totalProcessed;
        ThreadPoolExecutor executor = (ThreadPoolExecutor) executorService;
        
        logger.info(String.format(
            "Batch metrics - Size: %d, Duration: %dms, Total: %d, Avg time per record: %.2fms\n" +
            "Thread pool - Active: %d, Pool size: %d, Queue size: %d",
            batchSize, duration, totalProcessed, avgTimePerRecord,
            executor.getActiveCount(),
            executor.getPoolSize(),
            executor.getQueue().size()
        ));

        // Log connection pool metrics if available
        if (dataSource != null && dataSource.getHikariPoolMXBean() != null) {
            logger.info(String.format(
                "Connection pool - Active: %d, Idle: %d, Total: %d, Waiting: %d",
                dataSource.getHikariPoolMXBean().getActiveConnections(),
                dataSource.getHikariPoolMXBean().getIdleConnections(),
                dataSource.getHikariPoolMXBean().getTotalConnections(),
                dataSource.getHikariPoolMXBean().getThreadsAwaitingConnection()
            ));
        }
    }

    @Override
    public void close() {
        boolean terminated = false;
        executorService.shutdown();
        try {
            // Wait for existing tasks to terminate
            terminated = executorService.awaitTermination(60, TimeUnit.SECONDS);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        } finally {
            if (!terminated) {
                executorService.shutdownNow();
            }
        }
        
        // Clean up thread locals
        queryBuilder.remove();
        jdbcTemplate.remove();
        
        logger.info("Writer closed. Processed total records: " + totalProcessed.get());
    }
}

// application.properties file

# Existing configurations
server.servlet.context-path=/api/entity-service
spring.application.name=entity-service
spring.batch.job.enabled=false

# JPA and Database Configuration
spring.jpa.hibernate.ddl-auto=none
spring.datasource.driver-class-name=oracle.jdbc.OracleDriver
spring.jpa.show-sql= true
spring.transaction.default-timeout=120
spring.datasource.url=${ENTITY_DATASOURCE_URL}
spring.datasource.username=${ENTITY_DATASOURCE_USERNAME}
spring.datasource.password=${ENTITY_DATASOURCE_PASSWORD}

# Server SSL Configuration
server.port=8443
server.ssl.keystore=${KEYSTORE_FILE}
server.ssl.key-store-password=${KEYSTORE_PASS}
server.ssl.key-store-type=${KEYSTORE_TYPE}
server.ssl.key-alias=${KEYSTORE_ALIAS}
server.ssl.key-password=${KEYSTORE_PASS}

# CORS Configuration
cors.allowed.origins=http://entity-ui-sbse-als-dev.apps.ecpdevtest.tcc.abc.gov

# New HikariCP Connection Pool Configuration
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.minimum-idle=5
spring.datasource.hikari.idle-timeout=300000
spring.datasource.hikari.connection-timeout=20000
spring.datasource.hikari.max-lifetime=1200000
spring.datasource.hikari.auto-commit=true
spring.datasource.hikari.validation-timeout=5000

# Batch Processing Configuration
batch.size=1000
batch.thread.pool.size=8
batch.chunk.size=500
batch.retry.max.attempts=3
batch.retry.backoff.period=1000

# Performance Tuning
spring.jpa.properties.hibernate.jdbc.batch_size=50
spring.jpa.properties.hibernate.order_inserts=true
spring.jpa.properties.hibernate.order_updates=true
spring.jpa.properties.hibernate.batch_versioned_data=true
spring.jpa.properties.hibernate.jdbc.batch_versioned_data=true
spring.jpa.properties.hibernate.connection.provider_disables_autocommit=true

# Statement Caching
spring.datasource.hikari.data-source-properties.cachePrepStmts=true
spring.datasource.hikari.data-source-properties.prepStmtCacheSize=250
spring.datasource.hikari.data-source-properties.prepStmtCacheSqlLimit=2048
spring.datasource.hikari.data-source-properties.useServerPrepStmts=true

# Logging Configuration
logging.level.com.abc.sbse.os.ts.csp.alsentity=INFO
logging.level.org.springframework.jdbc.core=DEBUG
logging.level.com.zaxxer.hikari=INFO
